{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03e98fba-cad9-4a35-a2c9-eb34d7e6cb43",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "print(\"Num GPUs Available: \", len(tf.config.experimental.list_physical_devices('GPU')))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2d1af81-29f4-42c5-97c2-df3a3d3f5349",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "gpus = tf.config.list_physical_devices('GPU')\n",
    "if gpus:\n",
    "    for gpu in gpus:\n",
    "        print(f\"Name: {gpu.name}, Type: {gpu.device_type}\")\n",
    "else:\n",
    "    print(\"No GPU devices found.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "213615d7-c66a-4811-a869-9406c0492dd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pandas as pd\n",
    "\n",
    "# # Define the C code snippets\n",
    "# code_snippet_1 = \"\"\"\n",
    "# #include <stdio.h>\n",
    "# // This is a single-line comment\n",
    "# /* This is a\n",
    "#    multi-line comment */\n",
    "# #define MAX(x, y) ((x) > (y) ? (x) : (y))\n",
    "\n",
    "# int main() {\n",
    "#     // Print the maximum of two numbers\n",
    "#     int a = 10;\n",
    "#     int b = 20;\n",
    "#     printf(\"Maximum is %d\\n\", MAX(a, b));\n",
    "#     return 0;\n",
    "# }\n",
    "# \"\"\"\n",
    "\n",
    "# code_snippet_2 = \"\"\"\n",
    "# float x = 3.14;\n",
    "# int y = 42; \n",
    "# [0,1,0,1,0,2,3]\n",
    "# int main() {\n",
    "#     int x = 10;\n",
    "#     if(x > 5){\n",
    "#         printf(\"Hello, World!\");\n",
    "#     }\n",
    "# }\n",
    "# \"\"\"\n",
    "\n",
    "# # Create a DataFrame\n",
    "# df = pd.DataFrame({\n",
    "#     'code': [code_snippet_1, code_snippet_2]\n",
    "# })\n",
    "\n",
    "# # Display the DataFrame\n",
    "# df.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5057106d-7bc6-49e0-ab71-e2132875ea6f",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-07-17 16:10:51.635767: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX_VNNI FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-07-17 16:10:51.703029: I tensorflow/core/util/util.cc:169] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2024-07-17 16:10:51.706309: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory\n",
      "2024-07-17 16:10:51.706320: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n",
      "2024-07-17 16:10:51.723457: E tensorflow/stream_executor/cuda/cuda_blas.cc:2981] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2024-07-17 16:10:52.125407: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory\n",
      "2024-07-17 16:10:52.125471: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory\n",
      "2024-07-17 16:10:52.125476: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n"
     ]
    }
   ],
   "source": [
    "import code_tokenize\n",
    "import re\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "import os\n",
    "import pickle\n",
    "import time\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb404448-4f70-4ab0-b9eb-0d8593d996f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"/home/irshad/Documents/Vuldetect_Source_code/diversevul_dataset/diversevul.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3235bb7-069d-46f5-8b98-b22bf5ade2a2",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de77d882-fefc-4f4c-a6db-b7d72d1db27d",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "df.target.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "368cf2ff-565e-4a4b-a567-1217b552218d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def preprocessing(c_program):\n",
    "    # Convert non-string inputs to string\n",
    "    if not isinstance(c_program, str):\n",
    "        c_program = str(c_program)\n",
    "    \n",
    "    # Apply preprocessing steps directly to the entire program string\n",
    "    preprocessed_program = re.sub(r'\\/\\*[\\s\\S]*?\\*\\/|\\/\\/.*', '', c_program)  # Remove comments\n",
    "    preprocessed_program = re.sub(r'#.*', '', preprocessed_program)            # Remove preprocessor directives\n",
    "    preprocessed_program = preprocessed_program.replace('\\\\\\n', '')             # Remove unnecessary backslashes\n",
    "    preprocessed_program = preprocessed_program.replace('\\t', '')              # Remove unnecessary indentation\n",
    "    preprocessed_program = preprocessed_program.strip()                         # Remove leading/trailing whitespace\n",
    "    return preprocessed_program"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "136ce855-856e-4ee6-83c9-72add7bbf1c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['func']= df['func'].apply(preprocessing)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "751ca9c8-816f-4e15-813b-6365b01973f0",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def my_tokenizer(text):\n",
    "    tokens = code_tokenize.tokenize(text, lang=\"c\", syntax_error=\"ignore\")\n",
    "    tokens_list = [str(x) for x in tokens]\n",
    "    return tokens_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d42f8984-c126-4480-a269-e1de8fde5f9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#df['code'] = df['code'].apply(my_tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e84cc12d-81ab-4933-a6ba-819d211fcd4e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/irshad/anaconda3/envs/tensorflow_env/lib/python3.9/site-packages/tree_sitter/__init__.py:36: FutureWarning: Language(path, name) is deprecated. Use Language(ptr, name) instead.\n",
      "  warn(\"{} is deprecated. Use {} instead.\".format(old, new), FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "# tokenize the data that can be used by embeddings\n",
    "#import tensorflow as tf\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "import os\n",
    "import pickle\n",
    "import time\n",
    "\n",
    "\n",
    "tokenizer_path = \"/home/irshad/Documents/Vuldetect/tokenizer/tokenizer.pkl\"\n",
    "retrain = False\n",
    "if os.path.exists(tokenizer_path) and not retrain:\n",
    "    print(\"[INFO] Loading saved tokenizer\")\n",
    "    with open(tokenizer_path, \"rb\") as tokenizer_file:\n",
    "        tokenizer = pickle.load(tokenizer_file)\n",
    "else:\n",
    "    t0 = time.time()\n",
    "    print(\"[INFO] fitting tokenizer on TRAIN data...\")\n",
    "    tokenizer = Tokenizer(lower=False, analyzer = my_tokenizer)  # used in the research paper\n",
    "    tokenizer.fit_on_texts(X_train.apply(lambda x: np.str_(x)))\n",
    "    # save the tokenizer for future use\n",
    "    with open(tokenizer_path, \"wb\") as tokenizer_file:\n",
    "        pickle.dump(tokenizer, tokenizer_file, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "        # pickle.HIGHEST_PROTOCOL is highest protocol version available\n",
    "    print(f\"time taken to fit and save the tokenizer: {time()-t0} s\")   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ecf9913-8fd0-4c26-a98b-69ce9282efb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#sequences = tokenizer.fit_on_texts_to_sequences(text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "40dc1cdd-8714-429f-9b1f-88283cdda27f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word index:\n",
      " {';': 1, ',': 2, 'int': 3, '(': 4, ')': 5, '=': 6, '\"': 7, '0': 8, '{': 9, '}': 10, 'x': 11, 'main': 12, 'a': 13, '10': 14, 'b': 15, 'printf': 16, '1': 17, '20': 18, 'Maximum is %d': 19, 'MAX': 20, 'return': 21, 'float': 22, '3.14': 23, 'y': 24, '42': 25, '[': 26, '2': 27, '3': 28, ']': 29, 'if': 30, '>': 31, '5': 32, 'Hello, World!': 33}\n",
      "maximum length:  51\n",
      "\n",
      "Training sequences:\n",
      " [[3, 12, 4, 5, 9, 3, 13, 6, 14, 1, 3, 15, 6, 18, 1, 16, 4, 7, 19, 7, 2, 20, 4, 13, 2, 15, 5, 5, 1, 21, 8, 1, 10], [22, 11, 6, 23, 1, 3, 24, 6, 25, 1, 26, 8, 2, 17, 2, 8, 2, 17, 2, 8, 2, 27, 2, 28, 29, 3, 12, 4, 5, 9, 3, 11, 6, 14, 1, 30, 4, 11, 31, 32, 5, 9, 16, 4, 7, 33, 7, 5, 1, 10, 10]]\n",
      "\n",
      "Padded training sequences:\n",
      " [[ 3 12  4  5  9  3 13  6 14  1  3 15  6 18  1 16  4  7 19  7  2 20  4 13\n",
      "   2 15  5  5  1 21  8  1 10  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
      "   0  0]\n",
      " [22 11  6 23  1  3 24  6 25  1 26  8  2 17  2  8  2 17  2  8  2 27  2 28\n",
      "  29  3 12  4  5  9  3 11  6 14  1 30  4 11 31 32  5  9 16  4  7 33  7  5\n",
      "   1 10]]\n",
      "\n",
      "Padded training shape: (2, 50)\n",
      "Training sequences data type: <class 'list'>\n",
      "Padded Training sequences data type: <class 'numpy.ndarray'>\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Get our training data word index\n",
    "word_index = tokenizer.word_index\n",
    "\n",
    "# Encode training data sentences into sequences\n",
    "train_sequences = tokenizer.texts_to_sequences(df['code'])\n",
    "\n",
    "# Get max training sequence length\n",
    "maxlen = max([len(x) for x in train_sequences])\n",
    "\n",
    "# Pad the training sequences\n",
    "train_padded = pad_sequences(train_sequences, padding='post', truncating='post', maxlen=50)\n",
    "\n",
    "# Output the results of our work\n",
    "print(\"Word index:\\n\", word_index)\n",
    "print('maximum length: ', maxlen)\n",
    "print(\"\\nTraining sequences:\\n\", train_sequences)\n",
    "print(\"\\nPadded training sequences:\\n\", train_padded)\n",
    "print(\"\\nPadded training shape:\", train_padded.shape)\n",
    "print(\"Training sequences data type:\", type(train_sequences))\n",
    "print(\"Padded Training sequences data type:\", type(train_padded))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "ad026926-dbde-4b68-8e6b-d9c308403540",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['int', 'main', '(', ')', '{', 'int', 'a', '=', '10', ';', 'int', 'b', '=', '20', ';', 'printf', '(', '\"', 'Maximum is %d', '\"', ',', 'MAX', '(', 'a', ',', 'b', ')', ')', ';', 'return', '0', ';', '}']\n",
      "['float', 'x', '=', '3.14', ';', 'int', 'y', '=', '42', ';', '[', '0', ',', '1', ',', '0', ',', '1', ',', '0', ',', '2', ',', '3', ']', 'int', 'main', '(', ')', '{', 'int', 'x', '=', '10', ';', 'if', '(', 'x', '>', '5', ')', '{', 'printf', '(', '\"', 'Hello, World!', '\"', ')', ';', '}', '}']\n"
     ]
    }
   ],
   "source": [
    "# Reverse the word index to get index -> word mapping\n",
    "index_word = {index: word for word, index in word_index.items()}\n",
    "tokens_list = [[index_word.get(idx, '[UNK]') for idx in seq] for seq in train_sequences]\n",
    "# Print the tokens\n",
    "for i, text_tokens in enumerate(tokens_list):\n",
    "    print(text_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4bf676dd-c3e0-4b6b-b577-4610f16eb2fd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['int', 'main', '(', ')', '{', 'int', 'a', '=', '10', ';', 'int', 'b', '=', '20', ';', 'printf', '(', '\"', 'Maximum is %d', '\"', ',', 'MAX', '(', 'a', ',', 'b', ')', ')', ';', 'return', '0', ';', '}']\n",
      "['float', 'x', '=', '3.14', ';', 'int', 'y', '=', '42', ';', '[', '0', ',', '1', ',', '0', ',', '1', ',', '0', ',', '2', ',', '3', ']', 'int', 'main', '(', ')', '{', 'int', 'x', '=', '10', ';', 'if', '(', 'x', '>', '5', ')', '{', 'printf', '(', '\"', 'Hello, World!', '\"', ')', ';', '}', '}']\n"
     ]
    }
   ],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e2b80497-e7b4-497b-81a6-ec180708d495",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim\n",
    "from gensim.models import Word2Vec\n",
    "word2vec_model=gensim.models.Word2Vec(\n",
    "    sg=1,\n",
    "    window=10,\n",
    "    min_count=2,\n",
    "    workers=16\n",
    ")\n",
    "word2vec_model.build_vocab(tokens_list)\n",
    "start_time = time.time()\n",
    "word2vec_model.train(sentences=tokens_list, total_examples = word2vec_model.corpus_count, epochs=word2vec_model.epochs)\n",
    "print(f'Time taken : {(time.time() - start_time) / 60:.2f} mins')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "e9b32904-1346-470e-a579-ed97387d7842",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import multiprocessing\n",
    "num_cores = multiprocessing.cpu_count()\n",
    "print(num_cores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12a27fee-2017-488c-b0ae-3dc08c1409f4",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "becc92d6-803c-498f-aad7-8c9d8cf369aa",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(17, 100)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.wv.get_normed_vectors().shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "1d4590bd-8825-4643-a377-91ac798b4838",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "y=word2vec_model.wv.index_to_key"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "33ef24bf-73f0-4641-8d30-63b5f545e17f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[';',\n",
       " ',',\n",
       " '(',\n",
       " ')',\n",
       " 'int',\n",
       " '=',\n",
       " '\"',\n",
       " '0',\n",
       " 'x',\n",
       " '{',\n",
       " '}',\n",
       " 'printf',\n",
       " 'b',\n",
       " '10',\n",
       " 'a',\n",
       " 'main',\n",
       " '1']"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bca6c01-d3d2-40cc-b61b-68b93e809092",
   "metadata": {},
   "outputs": [],
   "source": [
    "word2vec_model.wv.most_similar('')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dcb3d8fb-b760-4f98-9f72-299d29b1ee1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "word2vec_model.wv['printf']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcc87cc0-9d11-4027-aeaf-e4c7c1df8b71",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "import os\n",
    "import pickle\n",
    "import time\n",
    "\n",
    "\n",
    "tokenizer_path = \"/home/irshad/Documents/Vuldetect/tokenizer/tokenizer.pkl\"\n",
    "retrain = False\n",
    "if os.path.exists(tokenizer_path) and not retrain:\n",
    "    print(\"[INFO] Loading saved tokenizer\")\n",
    "    with open(tokenizer_path, \"rb\") as tokenizer_file:\n",
    "        tokenizer = pickle.load(tokenizer_file)\n",
    "else:\n",
    "    t0 = time.time()\n",
    "    print(\"[INFO] fitting tokenizer on TRAIN data...\")\n",
    "    tokenizer = Tokenizer(lower=False, analyzer = my_tokenizer)  # used in the research paper\n",
    "    tokenizer.fit_on_texts(X_train.apply(lambda x: np.str_(x)))\n",
    "    # save the tokenizer for future use\n",
    "    with open(tokenizer_path, \"wb\") as tokenizer_file:\n",
    "        pickle.dump(tokenizer, tokenizer_file, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "        # pickle.HIGHEST_PROTOCOL is highest protocol version available\n",
    "    print(f\"time taken to fit and save the tokenizer: {time()-t0} s\")   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a6b198f-d043-4e24-b1c0-e58b0a42480f",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
