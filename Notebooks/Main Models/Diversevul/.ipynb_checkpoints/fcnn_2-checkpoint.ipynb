{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1640ca17-368b-41d6-87f8-b84053ae8d43",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split, StratifiedKFold\n",
    "from sklearn.metrics import classification_report, confusion_matrix, f1_score,ConfusionMatrixDisplay\n",
    "\n",
    "\n",
    "os.environ['TF_FORCE_UNIFIED_MEMORY']='1'\n",
    "os.environ['XLA_PYTHON_CLIENT_MEM_FRACTION']='2.0'\n",
    "#os.environ['XLA_PYTHON_CLIENT_PREALLOCATE']='false'\n",
    "os.environ['TF_FORCE_GPU_ALLOW_GROWTH ']='true' # as I understood, this is redundant with the set_memory_growth part :)\n",
    "\n",
    "import tensorflow as tf    \n",
    "gpus = tf.config.list_physical_devices('GPU')\n",
    "if gpus:\n",
    "  try:\n",
    "    # Currently, memory growth needs to be the same across GPUs\n",
    "    for gpu in gpus:\n",
    "      print(gpu)\n",
    "      tf.config.experimental.set_memory_growth(gpu, True)\n",
    "    logical_gpus = tf.config.list_logical_devices('GPU')\n",
    "    print(len(gpus), \"Physical GPUs,\", len(logical_gpus), \"Logical GPUs\")\n",
    "  except RuntimeError as e:\n",
    "    # Memory growth must be set before GPUs have been initialized\n",
    "    print(e)\n",
    "\n",
    "\n",
    "from tensorflow.keras.models import Sequential, load_model\n",
    "from tensorflow.keras.layers import LSTM, Dense, Embedding, GRU, Bidirectional\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint, Callback\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "import keras.backend as K\n",
    "from keras.optimizers import Adam\n",
    "\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from keras.models import load_model\n",
    "from keras.utils import to_categorical\n",
    "from numpy import dstack\n",
    "\n",
    "\n",
    "\n",
    "import pickle\n",
    "from gensim.models import Word2Vec\n",
    "\n",
    "def load_files(directory):\n",
    "    file_list = []\n",
    "    for root, dirs, files in os.walk(directory):\n",
    "        for file in files:\n",
    "            if file.endswith(\".c\"):\n",
    "                file_path = os.path.join(root, file)\n",
    "                with open(file_path, \"r\") as file_content:\n",
    "                    file_text = file_content.read()\n",
    "                file_list.append(file_text)\n",
    "    return file_list\n",
    "# Load training and test files for vulnerable and non-vulnerable classes\n",
    "def load_data():\n",
    "    dataset_dir = 'F://Development_//Deep Learning//Basic Dataset//BTP//1//Dataset//Raw//dataset 1//Dataset_raw'\n",
    "    vulnerable_dir = os.path.join(dataset_dir, 'Vulnerable')\n",
    "    non_vulnerable_dir = os.path.join(dataset_dir, 'Non_vulnerable')\n",
    "\n",
    "    vulnerable_train_files = load_files(os.path.join(vulnerable_dir, 'TRAIN'))\n",
    "    vulnerable_test_files = load_files(os.path.join(vulnerable_dir, 'TEST'))\n",
    "    non_vulnerable_train_files = load_files(os.path.join(non_vulnerable_dir, 'TRAIN'))\n",
    "    non_vulnerable_test_files = load_files(os.path.join(non_vulnerable_dir, 'TEST'))\n",
    "\n",
    "    train_codes = vulnerable_train_files + non_vulnerable_train_files\n",
    "    test_codes = vulnerable_test_files + non_vulnerable_test_files\n",
    "\n",
    "    # train_codes = train_codes+ test_codes\n",
    "\n",
    "    labels = np.concatenate((np.ones(len(vulnerable_train_files)), np.zeros(len(non_vulnerable_train_files))), axis=0)\n",
    "\n",
    "    X_train, X_test, y_train, y_test = train_test_split(train_codes, labels, test_size=0.3, random_state=42)\n",
    "\n",
    "    # Print lengths of all datasets\n",
    "    print(\"Length of X_train:\", len(X_train))\n",
    "    print(\"Length of X_test:\", len(X_test))\n",
    "    print(\"Length of y_train:\", len(y_train))\n",
    "    print(\"Length of y_test:\", len(y_test))\n",
    "    print(\"Length of test_codes:\", len(test_codes))\n",
    "\n",
    "\n",
    "\n",
    "    return X_train, X_test, y_train, y_test, test_codes\n",
    "\n",
    "\n",
    "def tokenize_sequences(X_train, X_test, test_codes):\n",
    "    tokenizer = Tokenizer(num_words=359414, split=' ')\n",
    "    tokenizer.fit_on_texts(X_train)\n",
    "    X_train_sequences = tokenizer.texts_to_sequences(X_train)\n",
    "    X_test_sequences = tokenizer.texts_to_sequences(X_test)\n",
    "    test_sequences = tokenizer.texts_to_sequences(test_codes)\n",
    "\n",
    "    average_sequence_length = int(np.mean([len(seq) for seq in X_train_sequences]))\n",
    "\n",
    "    X_train_padded = pad_sequences(X_train_sequences, maxlen=average_sequence_length)\n",
    "    X_test_padded = pad_sequences(X_test_sequences, maxlen=average_sequence_length)\n",
    "    test_padded = pad_sequences(test_sequences, maxlen=average_sequence_length)\n",
    "\n",
    "    return X_train_padded, X_test_padded, test_padded, tokenizer.word_index\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "724b669a-2d67-45ac-8a42-c225a0600eb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from keras.models import load_model\n",
    "from keras.utils import plot_model\n",
    "from keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "import sys\n",
    "import numpy as np\n",
    "from io import StringIO\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import os\n",
    "\n",
    "def load_all_models(i=0):\n",
    "    all_models = list()\n",
    "\n",
    "    folder_path = 'F://Development_//Deep Learning//Basic Dataset//BTP//1//Outputs//Models//LSTM//word2vec//'\n",
    "    model_files = [f for f in os.listdir(folder_path) if f.endswith('.keras')]\n",
    "    \n",
    "    for filename in model_files:\n",
    "        model_path = os.path.join(folder_path, filename)\n",
    "        model = load_model(model_path)\n",
    "        all_models.append(model)\n",
    "        print(f'> Loaded {model_path}')\n",
    "\n",
    "    return all_models\n",
    "\n",
    "\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "\n",
    "def stacked_dataset(members, inputX):\n",
    "    stackX = None\n",
    "    for model in members:\n",
    "        # make prediction\n",
    "        yhat = model.predict(inputX, verbose=0)\n",
    "        # stack predictions into [rows, members, probabilities]\n",
    "        if stackX is None:\n",
    "            stackX = yhat\n",
    "        else:\n",
    "            stackX = np.dstack((stackX, yhat))\n",
    "    \n",
    "    print(\"Sample of stackX before reshaping:\")\n",
    "    for i in range(min(3, stackX.shape[0])):\n",
    "        print(np.round(stackX[i], decimals=4))\n",
    "        \n",
    "    # flatten predictions to [rows, members x probabilities]\n",
    "    print(\"Shape of stackX before reshaping:\", stackX.shape)\n",
    "    stackX = stackX.reshape((stackX.shape[0], stackX.shape[1]*stackX.shape[2]))\n",
    "    print(\"Shape of stackX after reshaping:\", stackX.shape)\n",
    "    \n",
    "    print(\"Sample of stackX after reshaping:\")\n",
    "    for i in range(min(3, stackX.shape[0])):\n",
    "        print(np.round(stackX[i], decimals=4))\n",
    "\n",
    "    return stackX\n",
    "\n",
    "\n",
    "\n",
    "def fit_stacked_model(members, inputX, inputy, validation_split=0.1):\n",
    "    stackedX = stacked_dataset(members, inputX)\n",
    "    \n",
    "    model = Sequential([\n",
    "        Dense(128, activation='relu', input_shape=(stackedX.shape[1],)),\n",
    "        Dense(1, activation='sigmoid')\n",
    "    ])\n",
    "    model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "    history = model.fit(stackedX, inputy, epochs=50, batch_size=64, verbose=1, validation_split=validation_split)\n",
    "\n",
    "    plt.figure(figsize=(10, 5))\n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.plot(history.history['loss'], label='Training Loss')\n",
    "    plt.plot(history.history['val_loss'], label='Validation Loss')\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.title('Training and Validation Loss')\n",
    "    plt.legend()\n",
    "\n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.plot(history.history['accuracy'], label='Training Accuracy')\n",
    "    plt.plot(history.history['val_accuracy'], label='Validation Accuracy')\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.ylabel('Accuracy')\n",
    "    plt.title('Training and Validation Accuracy')\n",
    "    plt.legend()\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "    return model\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# def evaluate_model(X_test_padded, y_test, model):\n",
    "#     stackedX = stacked_dataset(members, inputX)\n",
    "#     y_pred_prob = model.predict(X_test_padded)\n",
    "#     y_pred = (y_pred_prob > 0.5).astype(int)\n",
    "#     report = classification_report(y_test, y_pred)\n",
    "#     confusion = confusion_matrix(y_test, y_pred)\n",
    "#     class_names = {0: \"Non-vulnerable\", 1: \"Vulnerable\"}  # Example mapping for binary classes\n",
    "#     labels = list(class_names.values())\n",
    "\n",
    "#     disp = ConfusionMatrixDisplay(confusion_matrix = confusion, display_labels = labels)\n",
    "#     disp.plot(cmap=plt.cm.Blues)\n",
    "#     plt.show()\n",
    "#     print(report)\n",
    "#     print(\"Confusion Matrix:\")\n",
    "#     print(confusion)\n",
    "    \n",
    "def stacked_prediction(members, model, inputX, y_test, threshold=0.5):\n",
    "    stackedX = stacked_dataset(members, inputX)\n",
    "    yhat = model.predict(stackedX)\n",
    "    yhat_binary = (yhat > threshold).astype(int)\n",
    "    report = classification_report(y_test, yhat_binary)\n",
    "\n",
    "    confusion = confusion_matrix(y_test, yhat_binary)\n",
    "    class_names = {0: \"Non-vulnerable\", 1: \"Vulnerable\"}  # Example mapping for binary classes\n",
    "    labels = list(class_names.values())\n",
    "\n",
    "    tn, fp, fn, tp = confusion.ravel()\n",
    "    fpr = fp / (fp + tn)\n",
    "    tpr = tp / (fn + tp)\n",
    "    \n",
    "    disp = ConfusionMatrixDisplay(confusion_matrix = confusion, display_labels = labels)\n",
    "    disp.plot(cmap=plt.cm.Blues)\n",
    "    plt.show()\n",
    "    print(report)\n",
    "    print(\"Confusion Matrix:\")\n",
    "    print(confusion)\n",
    "    print(\"False Positive Rate (FPR):\", fpr)\n",
    "    print(\"False Negative Rate (TPR):\", tpr)\n",
    "    \n",
    "    return yhat_binary\n",
    "\n",
    "X_train, X_test, y_train, y_test, test_codes = load_data()\n",
    "\n",
    "# Tokenize the sequences\n",
    "X_train_padded, X_test_padded, test_padded, word_index = tokenize_sequences(X_train, X_test, test_codes)\n",
    "\n",
    "import itertools\n",
    "\n",
    "members = load_all_models(1)\n",
    "# for model in members:\n",
    "#    _, acc = model.evaluate(X_test_padded, y_test, verbose=0)\n",
    "#    print('Model Accuracy: %.3f' % acc\n",
    "\n",
    "# fit stacked model using the ensemble\n",
    "model = fit_stacked_model(members, X_train_padded, y_train)\n",
    "\n",
    "# evaluate model on test set\n",
    "yhat = stacked_prediction(members, model, X_test_padded, y_test)\n",
    "print(\"Sample of y_test:\")\n",
    "print(y_test[:5])  # Print the first 5 elements of y_test\n",
    "\n",
    "print(\"Sample of yhat:\")\n",
    "print(yhat[:5])  # Print the first 5 elements of yhat\n",
    "\n",
    "\n",
    "acc = accuracy_score(y_test, yhat)\n",
    "print('Stacked Test Accuracy: %.3f' % acc)\n",
    "# evaluate_model(X_test_padded, y_test, model)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
