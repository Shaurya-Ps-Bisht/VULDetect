{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0a05fbe0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-01-24 10:21:34.428862: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F AVX512_VNNI FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-01-24 10:21:34.578578: I tensorflow/core/util/port.cc:104] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2024-01-24 10:21:35.307448: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: :/home/shaurya/miniconda3/envs/tf/lib/\n",
      "2024-01-24 10:21:35.307546: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: :/home/shaurya/miniconda3/envs/tf/lib/\n",
      "2024-01-24 10:21:35.307555: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split, StratifiedKFold\n",
    "from sklearn.metrics import classification_report, confusion_matrix, f1_score\n",
    "import tensorflow as tf\n",
    "tf.data.experimental.enable_debug_mode()\n",
    "tf.config.run_functions_eagerly(True)\n",
    "tf.get_logger().setLevel(tf.compat.v1.logging.DEBUG)\n",
    "from tensorflow.keras.models import Sequential, load_model\n",
    "from tensorflow.keras.layers import LSTM, Dense, Embedding, GRU, Bidirectional\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint, Callback\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "import keras.backend as K\n",
    "from keras.optimizers import Adam\n",
    "\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from keras.models import load_model\n",
    "from keras.utils import to_categorical\n",
    "from numpy import dstack\n",
    "\n",
    "\n",
    "\n",
    "import pickle\n",
    "from gensim.models import Word2Vec\n",
    "\n",
    "def load_files(directory):\n",
    "    file_list = []\n",
    "    for root, dirs, files in os.walk(directory):\n",
    "        for file in files:\n",
    "            if file.endswith(\".c\"):\n",
    "                file_path = os.path.join(root, file)\n",
    "                with open(file_path, \"r\") as file_content:\n",
    "                    file_text = file_content.read()\n",
    "                file_list.append(file_text)\n",
    "    return file_list\n",
    "# Load training and test files for vulnerable and non-vulnerable classes\n",
    "def load_data():\n",
    "    dataset_dir = '/home/shaurya/BTP/Dataset/Code/C'\n",
    "    vulnerable_dir = os.path.join(dataset_dir, 'Vulnerable')\n",
    "    non_vulnerable_dir = os.path.join(dataset_dir, 'Non_vulnerable')\n",
    "\n",
    "    vulnerable_train_files = load_files(os.path.join(vulnerable_dir, 'TRAIN'))\n",
    "    vulnerable_test_files = load_files(os.path.join(vulnerable_dir, 'TEST'))\n",
    "    non_vulnerable_train_files = load_files(os.path.join(non_vulnerable_dir, 'TRAIN'))\n",
    "    non_vulnerable_test_files = load_files(os.path.join(non_vulnerable_dir, 'TEST'))\n",
    "\n",
    "    train_codes = vulnerable_train_files + non_vulnerable_train_files\n",
    "    test_codes = vulnerable_test_files + non_vulnerable_test_files\n",
    "\n",
    "    labels = np.concatenate((np.ones(len(vulnerable_train_files)), np.zeros(len(non_vulnerable_train_files))), axis=0)\n",
    "\n",
    "    X_train, X_test, y_train, y_test = train_test_split(train_codes, labels, test_size=0.3, random_state=42)\n",
    "\n",
    "    return X_train, X_test, y_train, y_test, test_codes\n",
    "\n",
    "\n",
    "def tokenize_sequences(X_train, X_test, test_codes):\n",
    "    tokenizer = Tokenizer(num_words=10000, split='\\n')\n",
    "    tokenizer.fit_on_texts(X_train)\n",
    "    X_train_sequences = tokenizer.texts_to_sequences(X_train)\n",
    "    X_test_sequences = tokenizer.texts_to_sequences(X_test)\n",
    "    test_sequences = tokenizer.texts_to_sequences(test_codes)\n",
    "\n",
    "    average_sequence_length = int(np.mean([len(seq) for seq in X_train_sequences]))\n",
    "\n",
    "    X_train_padded = pad_sequences(X_train_sequences, maxlen=average_sequence_length)\n",
    "    X_test_padded = pad_sequences(X_test_sequences, maxlen=average_sequence_length)\n",
    "    test_padded = pad_sequences(test_sequences, maxlen=average_sequence_length)\n",
    "\n",
    "    return X_train_padded, X_test_padded, test_padded, tokenizer.word_index\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "06312a9f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Accuracy:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-01-24 09:47:18.428501: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F AVX512_VNNI FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-01-24 09:47:23.429250: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1613] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 6748 MB memory:  -> device: 0, name: NVIDIA GeForce GTX 1080 Ti, pci bus id: 0000:1b:00.0, compute capability: 6.1\n",
      "2024-01-24 09:47:23.430173: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1613] Created device /job:localhost/replica:0/task:0/device:GPU:1 with 3946 MB memory:  -> device: 1, name: NVIDIA GeForce GTX 1080 Ti, pci bus id: 0000:1d:00.0, compute capability: 6.1\n",
      "2024-01-24 09:47:23.431024: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1613] Created device /job:localhost/replica:0/task:0/device:GPU:2 with 4931 MB memory:  -> device: 2, name: NVIDIA GeForce GTX 1080 Ti, pci bus id: 0000:1e:00.0, compute capability: 6.1\n",
      "2024-01-24 09:47:23.431922: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1613] Created device /job:localhost/replica:0/task:0/device:GPU:3 with 10405 MB memory:  -> device: 3, name: NVIDIA GeForce GTX 1080 Ti, pci bus id: 0000:3d:00.0, compute capability: 6.1\n",
      "2024-01-24 09:47:23.432814: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1613] Created device /job:localhost/replica:0/task:0/device:GPU:4 with 10405 MB memory:  -> device: 4, name: NVIDIA GeForce GTX 1080 Ti, pci bus id: 0000:3f:00.0, compute capability: 6.1\n",
      "2024-01-24 09:47:23.433722: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1613] Created device /job:localhost/replica:0/task:0/device:GPU:5 with 10405 MB memory:  -> device: 5, name: NVIDIA GeForce GTX 1080 Ti, pci bus id: 0000:41:00.0, compute capability: 6.1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DEBUG:tensorflow:Layer gru_1 will use cuDNN kernels when running on GPU.\n",
      "> Loaded /home/shaurya/BTP/Outputs/Models/LSTM/word2vec/GRU_w2v.keras\n",
      "DEBUG:tensorflow:Layer lstm_2 will use cuDNN kernels when running on GPU.\n",
      "DEBUG:tensorflow:Layer lstm_2 will use cuDNN kernels when running on GPU.\n",
      "DEBUG:tensorflow:Layer lstm_2 will use cuDNN kernels when running on GPU.\n",
      "DEBUG:tensorflow:Layer lstm_3 will use cuDNN kernels when running on GPU.\n",
      "DEBUG:tensorflow:Layer lstm_3 will use cuDNN kernels when running on GPU.\n",
      "DEBUG:tensorflow:Layer lstm_3 will use cuDNN kernels when running on GPU.\n",
      "> Loaded /home/shaurya/BTP/Outputs/Models/LSTM/word2vec/BLSTM_w2v.keras\n",
      "DEBUG:tensorflow:Layer LSTM_W2V_1 will use cuDNN kernels when running on GPU.\n",
      "DEBUG:tensorflow:Layer LSTM_W2V_2 will use cuDNN kernels when running on GPU.\n",
      "> Loaded /home/shaurya/BTP/Outputs/Models/LSTM/word2vec/LSTM_w2v.keras\n",
      "DEBUG:tensorflow:Layer lstm will use cuDNN kernels when running on GPU.\n",
      "DEBUG:tensorflow:Layer lstm will use cuDNN kernels when running on GPU.\n",
      "DEBUG:tensorflow:Layer lstm will use cuDNN kernels when running on GPU.\n",
      "DEBUG:tensorflow:Layer lstm_1 will use cuDNN kernels when running on GPU.\n",
      "DEBUG:tensorflow:Layer lstm_1 will use cuDNN kernels when running on GPU.\n",
      "DEBUG:tensorflow:Layer lstm_1 will use cuDNN kernels when running on GPU.\n",
      "> Loaded /home/shaurya/BTP/Outputs/Models/LSTM/word2vec/BLSTM_ft.keras\n",
      "DEBUG:tensorflow:Layer LSTM_FT_1 will use cuDNN kernels when running on GPU.\n",
      "DEBUG:tensorflow:Layer LSTM_FT_2 will use cuDNN kernels when running on GPU.\n",
      "> Loaded /home/shaurya/BTP/Outputs/Models/LSTM/word2vec/LSTM_ft.keras\n",
      "DEBUG:tensorflow:Layer GRU_FT will use cuDNN kernels when running on GPU.\n",
      "> Loaded /home/shaurya/BTP/Outputs/Models/LSTM/word2vec/GRU_ft.keras\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-01-24 09:47:28.021941: I tensorflow/compiler/xla/stream_executor/cuda/cuda_dnn.cc:428] Loaded cuDNN version 8100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Accuracy: 0.970\n",
      "Model Accuracy: 0.964\n",
      "Model Accuracy: 0.968\n",
      "Model Accuracy: 0.949\n",
      "Model Accuracy: 0.949\n",
      "Model Accuracy: 0.954\n",
      "Stacked Test Accuracy: 0.975\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from keras.models import load_model\n",
    "\n",
    "def load_all_models():\n",
    "    all_models = list()\n",
    "    \n",
    "    folder_path = '/home/shaurya/BTP/Outputs/Models/LSTM/word2vec'\n",
    "    model_files = [f for f in os.listdir(folder_path) if f.endswith('.keras')]\n",
    "    \n",
    "    for filename in model_files:\n",
    "        model_path = os.path.join(folder_path, filename)\n",
    "        model = load_model(model_path)\n",
    "        all_models.append(model)\n",
    "        \n",
    "        print(f'> Loaded {model_path}')\n",
    "    \n",
    "    return all_models\n",
    "def stacked_dataset(members, inputX):\n",
    "    stackX = None\n",
    "    for model in members:\n",
    "        # make prediction\n",
    "        yhat = model.predict(inputX, verbose=0)\n",
    "        # stack predictions into [rows, members, probabilities]\n",
    "        if stackX is None:\n",
    "            stackX = yhat\n",
    "        else:\n",
    "            stackX = dstack((stackX, yhat))\n",
    "    # flatten predictions to [rows, members x probabilities]\n",
    "    stackX = stackX.reshape((stackX.shape[0], stackX.shape[1]*stackX.shape[2]))\n",
    "    return stackX\n",
    "\n",
    "\n",
    "def fit_stacked_model(members, inputX, inputy):\n",
    "    # create dataset using ensemble\n",
    "    stackedX = stacked_dataset(members, inputX)\n",
    "    # fit standalone model\n",
    "    model = LogisticRegression()\n",
    "    model.fit(stackedX, inputy)\n",
    "    return model\n",
    "\n",
    "# make a prediction with the stacked model\n",
    "def stacked_prediction(members, model, inputX):\n",
    "    # create dataset using ensemble\n",
    "    stackedX = stacked_dataset(members, inputX)\n",
    "    # make a prediction\n",
    "    yhat = model.predict(stackedX)\n",
    "    return yhat\n",
    "\n",
    "X_train, X_test, y_train, y_test, test_codes = load_data()\n",
    "\n",
    "print('Model Accuracy:')\n",
    "# Tokenize the sequences\n",
    "X_train_padded, X_test_padded, test_padded, word_index = tokenize_sequences(X_train, X_test, test_codes)\n",
    "\n",
    "members = load_all_models()\n",
    "for model in members:\n",
    "#     testy_enc = to_categorical(y_test)\n",
    "    _, acc = model.evaluate(X_test_padded, y_test, verbose=0)\n",
    "    print('Model Accuracy: %.3f' % acc)\n",
    "# fit stacked model using the ensemble\n",
    "model = fit_stacked_model(members, X_test_padded, y_test)\n",
    "# evaluate model on test set\n",
    "yhat = stacked_prediction(members, model, X_test_padded)\n",
    "acc = accuracy_score(y_test, yhat)\n",
    "print('Stacked Test Accuracy: %.3f' % acc)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08a535db",
   "metadata": {},
   "source": [
    "#Integerated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "873cfc25",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DEBUG:tensorflow:Layer GRU_W2V will use cuDNN kernels when running on GPU.\n",
      "> Loaded /home/shaurya/BTP/Outputs/Models/LSTM/word2vec/GRU_w2v.keras\n",
      "DEBUG:tensorflow:Layer lstm_2 will use cuDNN kernels when running on GPU.\n",
      "DEBUG:tensorflow:Layer lstm_2 will use cuDNN kernels when running on GPU.\n",
      "DEBUG:tensorflow:Layer lstm_2 will use cuDNN kernels when running on GPU.\n",
      "DEBUG:tensorflow:Layer lstm_3 will use cuDNN kernels when running on GPU.\n",
      "DEBUG:tensorflow:Layer lstm_3 will use cuDNN kernels when running on GPU.\n",
      "DEBUG:tensorflow:Layer lstm_3 will use cuDNN kernels when running on GPU.\n",
      "> Loaded /home/shaurya/BTP/Outputs/Models/LSTM/word2vec/BLSTM_w2v.keras\n",
      "DEBUG:tensorflow:Layer LSTM_W2V_1 will use cuDNN kernels when running on GPU.\n",
      "DEBUG:tensorflow:Layer LSTM_W2V_2 will use cuDNN kernels when running on GPU.\n",
      "> Loaded /home/shaurya/BTP/Outputs/Models/LSTM/word2vec/LSTM_w2v.keras\n",
      "DEBUG:tensorflow:Layer lstm will use cuDNN kernels when running on GPU.\n",
      "DEBUG:tensorflow:Layer lstm will use cuDNN kernels when running on GPU.\n",
      "DEBUG:tensorflow:Layer lstm will use cuDNN kernels when running on GPU.\n",
      "DEBUG:tensorflow:Layer lstm_1 will use cuDNN kernels when running on GPU.\n",
      "DEBUG:tensorflow:Layer lstm_1 will use cuDNN kernels when running on GPU.\n",
      "DEBUG:tensorflow:Layer lstm_1 will use cuDNN kernels when running on GPU.\n",
      "> Loaded /home/shaurya/BTP/Outputs/Models/LSTM/word2vec/BLSTM_ft.keras\n",
      "DEBUG:tensorflow:Layer LSTM_FT_1 will use cuDNN kernels when running on GPU.\n",
      "DEBUG:tensorflow:Layer LSTM_FT_2 will use cuDNN kernels when running on GPU.\n",
      "> Loaded /home/shaurya/BTP/Outputs/Models/LSTM/word2vec/LSTM_ft.keras\n",
      "DEBUG:tensorflow:Layer GRU_FT will use cuDNN kernels when running on GPU.\n",
      "> Loaded /home/shaurya/BTP/Outputs/Models/LSTM/word2vec/GRU_ft.keras\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-01-24 10:23:13.024889: I tensorflow/compiler/xla/stream_executor/cuda/cuda_dnn.cc:428] Loaded cuDNN version 8100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stacked Test Accuracy: 0.975\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from keras.models import load_model\n",
    "from keras.utils import to_categorical\n",
    "from keras.utils import plot_model\n",
    "from keras.models import Model\n",
    "from keras.layers import Input, Dense, concatenate\n",
    "from numpy import argmax\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.datasets import make_blobs\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.utils import to_categorical\n",
    "from numpy import dstack\n",
    "\n",
    "import random\n",
    "import string\n",
    "\n",
    "def generate_random_name():\n",
    "    return ''.join(random.choices(string.ascii_letters + string.digits, k=10))\n",
    "\n",
    "def rename_nested_layers(model, filename, model_index):\n",
    "    for layer in model.layers:\n",
    "        original_name = layer.name\n",
    "        random_name = f\"{filename}_{model_index}_{generate_random_name()}\"\n",
    "        layer._name = random_name\n",
    "\n",
    "        if hasattr(layer, 'layers'):  # Check for nested layers\n",
    "            rename_nested_layers(layer, filename, model_index)  # Recursive call\n",
    "\n",
    "# The rest of your code remains unchanged\n",
    "\n",
    "def load_all_models():\n",
    "    all_models = list()\n",
    "\n",
    "    folder_path = '/home/shaurya/BTP/Outputs/Models/LSTM/word2vec'\n",
    "    model_files = [f for f in os.listdir(folder_path) if f.endswith('.keras')]\n",
    "\n",
    "    for model_index, filename in enumerate(model_files):\n",
    "        model_path = os.path.join(folder_path, filename)\n",
    "        model = load_model(model_path)\n",
    "\n",
    "        # Rename all layers recursively\n",
    "#         rename_nested_layers(model, filename, model_index)\n",
    "\n",
    "        all_models.append(model)\n",
    "        print(f'> Loaded {model_path}')\n",
    "\n",
    "    return all_models\n",
    "\n",
    "def define_stacked_model(members):\n",
    "    # Update all layers in all models to not be trainable\n",
    "    for i, model in enumerate(members):\n",
    "        for layer in model.layers:\n",
    "            # Make not trainable\n",
    "            layer.trainable = False\n",
    "            # Rename to avoid 'unique layer name' issue\n",
    "#             layer._name = 'ensemble_' + str(i + 1) + '_' + layer.name\n",
    "\n",
    "        # Rename the input layer\n",
    "#         model._name = 'ensemble_' + str(i + 1)\n",
    "\n",
    "    # Define multi-headed input with unique names\n",
    "    ensemble_visible = [model.input for model in members]\n",
    "\n",
    "    # Concatenate merge output from each model\n",
    "    ensemble_outputs = [model.output for model in members]\n",
    "    merge = concatenate(ensemble_outputs)\n",
    "\n",
    "    hidden = Dense(10, activation='relu')(merge)\n",
    "    output = Dense(2, activation='softmax')(hidden)\n",
    "\n",
    "    model = Model(inputs=ensemble_visible, outputs=output)\n",
    "\n",
    "    # Plot graph of ensemble\n",
    "#     plot_model(model, show_shapes=True, to_file='model_graph.png')\n",
    "\n",
    "    # Compile\n",
    "    model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "    return model\n",
    "\n",
    "def fit_stacked_model(model, inputX, inputy):\n",
    "    # prepare input data\n",
    "    X = [inputX for _ in range(len(model.input))]\n",
    "    \n",
    "    # encode output data\n",
    "    inputy_enc = to_categorical(inputy)\n",
    "    \n",
    "    # fit model\n",
    "    model.fit(X, inputy_enc, epochs=300, verbose=0)\n",
    "\n",
    "def predict_stacked_model(model, inputX):\n",
    "    # prepare input data\n",
    "    X = [inputX for _ in range(len(model.input))]\n",
    "    \n",
    "    # make prediction\n",
    "    return model.predict(X, verbose=0)\n",
    "\n",
    "X_train, X_test, y_train, y_test, test_codes = load_data()\n",
    "\n",
    "# Tokenize the sequences\n",
    "X_train_padded, X_test_padded, test_padded, word_index = tokenize_sequences(X_train, X_test, test_codes)\n",
    "\n",
    "members = load_all_models()\n",
    "\n",
    "# for model in members:\n",
    "#     _, acc = model.evaluate(X_test_padded, y_test, verbose=0)\n",
    "#     print('Model Accuracy: %.3f' % acc)\n",
    "\n",
    "# define ensemble model\n",
    "stacked_model = define_stacked_model(members)\n",
    "\n",
    "# fit stacked model on test dataset\n",
    "fit_stacked_model(stacked_model, X_test_padded, y_test)\n",
    "\n",
    "# make predictions and evaluate\n",
    "yhat = predict_stacked_model(stacked_model, X_test_padded)\n",
    "yhat = argmax(yhat, axis=1)\n",
    "acc = accuracy_score(y_test, yhat)\n",
    "print('Stacked Test Accuracy: %.3f' % acc)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7a3dbe0f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-01-24 10:22:33.316552: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F AVX512_VNNI FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-01-24 10:22:39.166754: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1613] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 6748 MB memory:  -> device: 0, name: NVIDIA GeForce GTX 1080 Ti, pci bus id: 0000:1b:00.0, compute capability: 6.1\n",
      "2024-01-24 10:22:39.167709: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1613] Created device /job:localhost/replica:0/task:0/device:GPU:1 with 3946 MB memory:  -> device: 1, name: NVIDIA GeForce GTX 1080 Ti, pci bus id: 0000:1d:00.0, compute capability: 6.1\n",
      "2024-01-24 10:22:39.168554: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1613] Created device /job:localhost/replica:0/task:0/device:GPU:2 with 4931 MB memory:  -> device: 2, name: NVIDIA GeForce GTX 1080 Ti, pci bus id: 0000:1e:00.0, compute capability: 6.1\n",
      "2024-01-24 10:22:39.169892: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1613] Created device /job:localhost/replica:0/task:0/device:GPU:3 with 10405 MB memory:  -> device: 3, name: NVIDIA GeForce GTX 1080 Ti, pci bus id: 0000:3d:00.0, compute capability: 6.1\n",
      "2024-01-24 10:22:39.171126: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1613] Created device /job:localhost/replica:0/task:0/device:GPU:4 with 10405 MB memory:  -> device: 4, name: NVIDIA GeForce GTX 1080 Ti, pci bus id: 0000:3f:00.0, compute capability: 6.1\n",
      "2024-01-24 10:22:39.172025: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1613] Created device /job:localhost/replica:0/task:0/device:GPU:5 with 10405 MB memory:  -> device: 5, name: NVIDIA GeForce GTX 1080 Ti, pci bus id: 0000:41:00.0, compute capability: 6.1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DEBUG:tensorflow:Layer GRU_W2V will use cuDNN kernels when running on GPU.\n",
      "> Loaded /home/shaurya/BTP/Outputs/Models/LSTM/word2vec/GRU_w2v.keras\n",
      "DEBUG:tensorflow:Layer lstm_2 will use cuDNN kernels when running on GPU.\n",
      "DEBUG:tensorflow:Layer lstm_2 will use cuDNN kernels when running on GPU.\n",
      "DEBUG:tensorflow:Layer lstm_2 will use cuDNN kernels when running on GPU.\n",
      "DEBUG:tensorflow:Layer lstm_3 will use cuDNN kernels when running on GPU.\n",
      "DEBUG:tensorflow:Layer lstm_3 will use cuDNN kernels when running on GPU.\n",
      "DEBUG:tensorflow:Layer lstm_3 will use cuDNN kernels when running on GPU.\n",
      "> Loaded /home/shaurya/BTP/Outputs/Models/LSTM/word2vec/BLSTM_w2v.keras\n",
      "DEBUG:tensorflow:Layer LSTM_W2V_1 will use cuDNN kernels when running on GPU.\n",
      "DEBUG:tensorflow:Layer LSTM_W2V_2 will use cuDNN kernels when running on GPU.\n",
      "> Loaded /home/shaurya/BTP/Outputs/Models/LSTM/word2vec/LSTM_w2v.keras\n",
      "DEBUG:tensorflow:Layer lstm will use cuDNN kernels when running on GPU.\n",
      "DEBUG:tensorflow:Layer lstm will use cuDNN kernels when running on GPU.\n",
      "DEBUG:tensorflow:Layer lstm will use cuDNN kernels when running on GPU.\n",
      "DEBUG:tensorflow:Layer lstm_1 will use cuDNN kernels when running on GPU.\n",
      "DEBUG:tensorflow:Layer lstm_1 will use cuDNN kernels when running on GPU.\n",
      "DEBUG:tensorflow:Layer lstm_1 will use cuDNN kernels when running on GPU.\n",
      "> Loaded /home/shaurya/BTP/Outputs/Models/LSTM/word2vec/BLSTM_ft.keras\n",
      "DEBUG:tensorflow:Layer LSTM_FT_1 will use cuDNN kernels when running on GPU.\n",
      "DEBUG:tensorflow:Layer LSTM_FT_2 will use cuDNN kernels when running on GPU.\n",
      "> Loaded /home/shaurya/BTP/Outputs/Models/LSTM/word2vec/LSTM_ft.keras\n",
      "DEBUG:tensorflow:Layer GRU_FT will use cuDNN kernels when running on GPU.\n",
      "> Loaded /home/shaurya/BTP/Outputs/Models/LSTM/word2vec/GRU_ft.keras\n",
      "Model 1:\n",
      "GRU_W2V_EL\n",
      "GRU_W2V\n",
      "GRU_W2V_OL\n",
      "Model 2:\n",
      "BLSTM_W2V_EL\n",
      "BLSTM_W2V_1\n",
      "BLSTM_W2V_2\n",
      "BLSTM_W2V_OL\n",
      "Model 3:\n",
      "LSTM_W2V_EL\n",
      "LSTM_W2V_1\n",
      "LSTM_W2V_2\n",
      "LSTM_W2V_OL\n",
      "Model 4:\n",
      "BLSTM_FT_EL\n",
      "BLSTM_FT_1\n",
      "BLSTM_FT_2\n",
      "BLSTM_FT_OL\n",
      "Model 5:\n",
      "LSTM_FT_EL\n",
      "LSTM_FT_1\n",
      "LSTM_FT_2\n",
      "LSTM_FT_OL\n",
      "Model 6:\n",
      "GRU_FT_EL\n",
      "GRU_FT\n",
      "GRU_FT_OL\n"
     ]
    }
   ],
   "source": [
    "def load_all_models():\n",
    "    all_models = list()\n",
    "\n",
    "    folder_path = '/home/shaurya/BTP/Outputs/Models/LSTM/word2vec'\n",
    "    model_files = [f for f in os.listdir(folder_path) if f.endswith('.keras')]\n",
    "\n",
    "    for model_index, filename in enumerate(model_files):\n",
    "        model_path = os.path.join(folder_path, filename)\n",
    "        model = load_model(model_path)\n",
    "\n",
    "        # Rename all layers recursively\n",
    "#         rename_nested_layers(model, filename, model_index)\n",
    "\n",
    "        all_models.append(model)\n",
    "        print(f'> Loaded {model_path}')\n",
    "\n",
    "    return all_models\n",
    "\n",
    "def list_models_and_layers(members):\n",
    "    for i, model in enumerate(members):\n",
    "        print(f\"Model {i + 1}:\")\n",
    "        print_layer_names(model)\n",
    "#         print(f\"  Layers:\")\n",
    "#         for layer in model.layers:\n",
    "#             print(f\"    {layer.name}\")\n",
    "#         print(\"\")\n",
    "\n",
    "def print_layer_names(model, indent=0):\n",
    "    for layer in model.layers:\n",
    "        print(\" \" * indent + layer.name)\n",
    "        if hasattr(layer, 'layers') and layer.layers:\n",
    "            print_layer_names(layer, indent + 4)\n",
    "\n",
    "# Assuming members is a list of loaded models\n",
    "members = load_all_models()  # Replace with your actual loading mechanism\n",
    "list_models_and_layers(members)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "473e0827",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
